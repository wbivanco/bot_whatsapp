from typing import Any
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.schema import HumanMessage, SystemMessage
from langchain.prompts import PromptTemplate

class LlmManager:
    """ Clase que maneja la carga y configuraci칩n de modelos de lenguaje y conversaci칩n. """  
    def __init__(self, llm_provider) -> None:
        self.llm_provider = llm_provider
    
    def initialice_llm_model(self, model_name, api_key, temperature=0):
        """ Carga el modelo y el api_key de OpenAI(pasados como par치metros obligatorios) y lo inicializa con la 
        temperatura deseada(por defecto es cero), se debe pasar el provider(empresa) del modelo, que indica que 
        clase de langchain se debe llamar.
        
        Args:
            model_name (str): Nombre del modelo.
            openai_api_key (str): Clave de API.
            temperature (float, optional): Temperatura del modelo. Por defecto es 0.
            system_prompt (str, optional): Prompt del sistema para inicializar el modelo. Por defecto es None.
    
        Returns:
            llm: Instancia del modelo inicializado. 
        """
       
        try:
            if self.llm_provider == "openai":
                llm = ChatOpenAI(                    
                    model_name=model_name, 
                    api_key=api_key,
                    temperature=temperature,                   
                )
        except Exception as e:
            print(f"Error al cargar el tipo de modelo LLM: {e}")
            llm = None    

        return llm
    
    def initialice_retriever(self, llm, stored_embeddings, search_type="similarity", num_result=1):
        """ Inicializa el modelo de recuperaci칩n de informaci칩n, pasando el modelo de lenguaje y los embeddings, 
        ademas se pueden pasar como par치metros el tipo de b칰squeda y la cantidad de resultados, ambos opcionales. """

        retriever = stored_embeddings.as_retriever(search_type=search_type, search_kwargs={"k": num_result})
        
        # Prompt template personalizado que fuerza al modelo a usar la informaci칩n de los documentos
        prompt_template = """Eres un asistente amigable y servicial para la Facultad de Humanidades de la Universidad Nacional de Catamarca.

REGLAS CR칈TICAS:
1. DEBES responder 칔NICAMENTE con la informaci칩n que est치 en los siguientes documentos de contexto
2. SIEMPRE responde en espa침ol argentino con tono c치lido y amigable
3. NUNCA digas "no s칠", "no tengo informaci칩n" o "no est치 en los documentos" sin haber analizado exhaustivamente el contexto
4. Si la informaci칩n no est치 exactamente en los documentos pero hay informaci칩n relacionada, comp치rtela de forma 칰til
5. Si realmente no hay informaci칩n relevante, ofrece ayuda con otros temas relacionados que s칤 tengas

CONTEXTO (informaci칩n de los documentos):
{context}

PREGUNTA DEL USUARIO: {question}

INSTRUCCIONES:
- Analiza cuidadosamente el contexto proporcionado
- Extrae la informaci칩n relevante para responder la pregunta
- Si encuentras informaci칩n relacionada aunque no sea exactamente lo que preguntan, comp치rtela
- Responde de forma clara, amigable y en espa침ol argentino
- Si tu respuesta es completa, term칤nala ah칤. Si crees que el usuario podr칤a tener m치s dudas, agrega al final: "游뱂 쯊e queda alguna duda sobre este tema?"

RESPUESTA:"""

        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        QA_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": PROMPT}
        )

        return QA_chain

    def get_response_retriever(self, QA_chain, user_message, history):
        """ Recibe la cadena con el llm y retriever, adem치s de la pregunta del usuario y el historial del chat, devuelve \
            la respuesta del bot, de donde saco la informaci칩n y el historial. """
        messages = [
            SystemMessage(
                content="""Eres un asistente amigable y servicial para la Facultad de Humanidades de la Universidad Nacional de Catamarca.

REGLAS IMPORTANTES:
1. SOLO responde con informaci칩n que est칠 en los documentos PDF proporcionados
2. SIEMPRE responde en castellano (espa침ol argentino)
3. Usa un tono c치lido, amigable y cercano como si hablaras con un amigo
4. Si no tienes la informaci칩n espec칤fica, NO digas simplemente "no s칠"
5. En lugar de "no s칠", responde de forma amigable como: "춰Hola! Esa informaci칩n espec칤fica no la tengo en mis documentos, pero puedo ayudarte con otros tr치mites de la facultad. 쯊e interesa saber sobre [menciona alg칰n tema relacionado que s칤 tengas]?"

TU ROL:
- Ayudar a estudiantes con tr치mites administrativos
- Brindar informaci칩n sobre gesti칩n acad칠mica
- Orientar sobre vida universitaria
- Ser emp치tico y comprensivo con las dudas de los j칩venes

RECUERDA: Eres un compa침ero que quiere ayudar, no un robot fr칤o. Usa emojis ocasionalmente y mant칠n un lenguaje cercano pero respetuoso.

DESPU칄S DE CADA RESPUESTA:
- Si tu respuesta es completa y clara, termina ah칤
- Si tu respuesta podr칤a no ser suficiente o si crees que el usuario podr칤a tener m치s dudas, agrega al final:
  "쯊e queda alguna duda sobre este tema o necesitas informaci칩n sobre algo m치s?"
- Usa emojis para hacer la pregunta m치s amigable, por ejemplo: "游뱂 쯊e queda alguna duda sobre este tema o necesitas informaci칩n sobre algo m치s?" """
            ),  # System prompt
        ]

        # Agregar el historial de mensajes
        for msg in history:            
            messages.append(HumanMessage(content=msg["user"]))
            messages.append(HumanMessage(content=msg["assistant"]))   

        # Agregar el mensaje actual del usuario
        messages.append(HumanMessage(content=user_message))

        response = QA_chain.invoke({"query": user_message, "messages": messages})
        bot_response = response["result"]
        source_documents = response["source_documents"]
        
        # Extraer fuentes con metadatos adicionales (ej. t칤tulo, enlace)
        sources = []
        for doc in source_documents:
            source_info = {
                'source': doc.metadata.get('source', 'Desconocido'),  # Fuente original
                'title': doc.metadata.get('title', 'Sin t칤tulo'),  # T칤tulo del documento
                'url': doc.metadata.get('url', ''),  # Enlace si est치 disponible
                'snippet': doc.page_content[:200]  # Primeros 200 caracteres del documento
            }
            sources.append(source_info)
  
        return bot_response, sources

    def get_response_retriever_without_memory(self, QA_chain, user_message):
        """ Recibe la cadena con el llm y retriever y la pregunta del usuario, devuelve solo la respuesta del bot. 
        El prompt template personalizado ya est치 configurado en initialice_retriever, as칤 que solo necesitamos pasar la query."""
        response = QA_chain.invoke({"query": user_message})
        bot_response = response["result"]
    
        return bot_response